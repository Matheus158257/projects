{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency–Inverse Document Frequency (TF-IDF) - Experimento\n",
    "\n",
    "Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "Este notebook apresenta:\n",
    "- como usar o [SDK](https://platiagro.github.io/sdk/) para carregar datasets, salvar modelos e outros artefatos.\n",
    "- como declarar parâmetros e usá-los para criar componentes reutilizáveis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare parâmetros e hiperparâmetros para o modelo\n",
    "Os componentes podem declarar (e usar) estes parâmetros como padrão:\n",
    "- dataset\n",
    "- target\n",
    "\n",
    "Use estes parâmetros para carregar/salvar conjutos de dados, modelos, métricas e figuras com a ajuda do [SDK da PlatIAgro](https://platiagro.github.io/sdk/). <br>\n",
    "É possível também declarar parâmetros personalizados para serem definidos ao executar um experimento. \n",
    "\n",
    "Selecione os hiperparâmetros e seus respectivos valores para serem usados ao treinar o modelo:\n",
    "- language\n",
    "\n",
    "Estes parâmetros são alguns dos oferecidos pela classe do modelo, você também pode utilizar outros existentes. <br>\n",
    "Dê uma olhada nos [parâmetros do modelo](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn-impute-simpleimputer) para mais informações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# parâmetros\n",
    "dataset = \"imdb.csv\" #@param {type:\"string\"}\n",
    "target = \"label\" #@param {type:\"feature\", label:\"Atributo alvo\", description:\"Seu modelo será treinado para prever os valores do alvo.\"}\n",
    "text = \"text\" #@param {type:\"string\", label:\"Texto alvo\", description:\"Nome da coluna do texto alvo pertencente ao dataset.\"}\n",
    "language = \"english\" #@param [\"portuguese\", \"english\"] {type:\"string\", label:\"Linguagem\", description:\"Linguagem da qual os stopwords pertencem. Deve ser a mesma utilizada no dataset.\"}\n",
    "\n",
    "# selected features to perform the model\n",
    "filter_type = \"incluir\" #@param [\"incluir\",\"remover\"] {type:\"string\",multiple:false,label:\"Modo de seleção das features\",description:\"Se deseja informar quais features deseja incluir no modelo, selecione a opção 'incluir'. Caso deseje informar as features que não devem ser utilizadas, selecione 'remover'. \"}\n",
    "model_features = \"text\" #@param {type:\"feature\",multiple:true,label:\"Features para incluir/remover no modelo\",description:\"Seu modelo será feito considerando apenas as features selecionadas. Caso nada seja especificado, todas as features serão utilizadas\"}\n",
    "\n",
    "# features to apply One Hot Encoder\n",
    "one_hot_features = \"\" #@param {type:\"feature\",multiple:true,label:\"Features para fazer codificação one-hot\", description: \"Seu modelo utilizará a codificação one-hot para as features selecionadas. As demais features categóricas serão codificadas utilizando a codificação ordinal.\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acesso ao conjunto de dados\n",
    "\n",
    "O conjunto de dados utilizado nesta etapa será o mesmo carregado através da plataforma.<br>\n",
    "O tipo da variável retornada depende do arquivo de origem:\n",
    "- [pandas.DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) para CSV e compressed CSV: .csv .csv.zip .csv.gz .csv.bz2 .csv.xz\n",
    "- [Binary IO stream](https://docs.python.org/3/library/io.html#binary-i-o) para outros tipos de arquivo: .jpg .wav .zip .h5 .parquet etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>Un-bleeping-believable! Meg Ryan doesn't even ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>This is a extremely well-made film. The acting...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>Every once in a long while a movie will come a...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>Name just says it all. I watched this movie wi...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>This movie succeeds at being one of the most u...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>negative</td>\n",
       "      <td>There are many different versions of this one ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>positive</td>\n",
       "      <td>Once upon a time Hollywood produced live-actio...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>negative</td>\n",
       "      <td>Wenders was great with Million $ Hotel.I don't...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>negative</td>\n",
       "      <td>Although a film with Bruce Willis is always wo...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>positive</td>\n",
       "      <td>A compelling, honest, daring, and unforgettabl...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        label                                               text  is_valid\n",
       "0    negative  Un-bleeping-believable! Meg Ryan doesn't even ...     False\n",
       "1    positive  This is a extremely well-made film. The acting...     False\n",
       "2    negative  Every once in a long while a movie will come a...     False\n",
       "3    positive  Name just says it all. I watched this movie wi...     False\n",
       "4    negative  This movie succeeds at being one of the most u...     False\n",
       "..        ...                                                ...       ...\n",
       "995  negative  There are many different versions of this one ...      True\n",
       "996  positive  Once upon a time Hollywood produced live-actio...      True\n",
       "997  negative  Wenders was great with Million $ Hotel.I don't...      True\n",
       "998  negative  Although a film with Bruce Willis is always wo...      True\n",
       "999  positive  A compelling, honest, daring, and unforgettabl...      True\n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(f'/tmp/data/{dataset}')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acesso aos metadados do conjunto de dados\n",
    "\n",
    "Utiliza a função `stat_dataset` do [SDK da PlatIAgro](https://platiagro.github.io/sdk/) para carregar metadados. <br>\n",
    "Por exemplo, arquivos CSV possuem `metadata['featuretypes']` para cada coluna no conjunto de dados (ex: categorical, numerical, or datetime)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from platiagro import stat_dataset\n",
    "\n",
    "metadata = stat_dataset(name=dataset)\n",
    "featuretypes = metadata[\"featuretypes\"]\n",
    "\n",
    "columns = df.columns.to_numpy()\n",
    "featuretypes = np.array(featuretypes)\n",
    "target_index = np.argwhere(columns == target)\n",
    "columns = np.delete(columns, target_index)\n",
    "featuretypes = np.delete(featuretypes, target_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remoção de linhas com valores faltantes no atributo alvo\n",
    "\n",
    "Caso haja linhas em que o atributo alvo contenha valores faltantes, é feita a remoção dos casos faltantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset = [target],inplace=True)\n",
    "y = df[target].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtragem das features \n",
    "\n",
    "Seleciona apenas as features que foram declaradas no parâmetro model_features. Se nenhuma feature for especificada, todo o conjunto de dados será utilizado para a modelagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "if filter_type == 'incluir':\n",
    "    if len(model_features) >= 1:\n",
    "        columns_index = (np.where(np.isin(columns,model_features)))[0]\n",
    "        columns_index.sort()\n",
    "        columns_to_filter = columns[columns_index]\n",
    "        featuretypes = featuretypes[columns_index]\n",
    "    else:\n",
    "        columns_to_filter = columns\n",
    "else:\n",
    "    if len(model_features) >= 1:\n",
    "        columns_index = (np.where(np.isin(columns,model_features)))[0]\n",
    "        columns_index.sort()\n",
    "        columns_to_filter = np.delete(columns,columns_index)\n",
    "        featuretypes = np.delete(featuretypes,columns_index)\n",
    "    else:\n",
    "        columns_to_filter = columns\n",
    "\n",
    "# keep the features selected\n",
    "df_model = df[columns_to_filter]\n",
    "X = df_model.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divisão do datset em subconjuntos de treino e teste\n",
    "\n",
    "Subconjunto de Treino: amostras de dados usado para treinar o modelo (``fit``). <br>\n",
    "Subconjunto de Teste: a amostra de dados usada para fornecer uma avaliação imparcial de um modelo adequado ao conjunto de dados de treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapeamento = {'positive': True, 'negative': False}\n",
    "y_train = [mapeamento.get(i, i) for i in y_train]\n",
    "y_test = [mapeamento.get(i, i) for i in y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Busca por stopwords\n",
    "\n",
    "Stopwords (ou palavras de parada) são palavras que geralmente se referem às mais comuns em um idioma ou em um corpus. <br>\n",
    "Elas podem ser ignoradas com segurança sem sacrificar o significado da frase, pois são palarvas que não agregram muito significado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk --quiet\n",
    "import nltk\n",
    "\n",
    "# Download stopwords from nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get a list of stopwords for the defined language\n",
    "stopwords = nltk.corpus.stopwords.words(language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processamento do texto\n",
    "\n",
    "Funções auxiliares para processamento dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from re import sub\n",
    "from collections import defaultdict\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "def tokenize_without_punctuation(text_list: list = None):\n",
    "    \"\"\"Tokenize without ponctuation.\n",
    "\n",
    "    Args:\n",
    "        text_list (list): a list of texts to be used.\n",
    "\n",
    "    Returns:\n",
    "        A list of tokenized text without punctuation.\n",
    "    \"\"\"\n",
    "    tokenize_list = list()\n",
    "    punctuation_pattern = \"[^a-zA-Z0-9áéíóúÁÉÍÓÚâêîôÂÊÎÔãõÃÕçÇ ]\"\n",
    "    html_tag_pattern = \"<.*?>\"\n",
    "\n",
    "    for text in text_list:\n",
    "        text = sub(html_tag_pattern, ' ', text[0])\n",
    "        tokenize_list.append(sub(punctuation_pattern, ' ', text).split(' '))\n",
    "\n",
    "    return tokenize_list\n",
    "\n",
    "\n",
    "def top_tokens_stopwords(token_list: list, percentage: float = 0.01):\n",
    "    \"\"\"Selects the most relevant stops words of the tokerized texts.\n",
    "\n",
    "    Args:\n",
    "        token_list (list): list of tokens.\n",
    "        percentage (float): percentage threshold.\n",
    "    \"\"\"\n",
    "    vocabulary = defaultdict(int)\n",
    "\n",
    "    for sample in token_list:\n",
    "        for token in sample:\n",
    "            vocabulary[token] += 1\n",
    "\n",
    "    all_tokens = sorted(vocabulary.items(), key=lambda token: token[1], reverse=True)\n",
    "    top_tokens = all_tokens[:int(len(all_tokens) * percentage)]\n",
    "\n",
    "    return [token[0] for token in top_tokens]\n",
    "\n",
    "\n",
    "def remove_specific_tokens(token_list: list, tokens_to_be_removed: list = None):\n",
    "    \"\"\"Removes specific tokens from a token list.\n",
    "\n",
    "    Args:\n",
    "        token_list (list): list of tokens from which other tokens will be removed.\n",
    "        tokens_to_be_removed (list): list of tokens that need to be removed.\n",
    "    \"\"\"\n",
    "    token_list_ = list()\n",
    "\n",
    "    if tokens_to_be_removed is None:\n",
    "        tokens_to_be_removed = top_tokens_stopwords(token_list)\n",
    "\n",
    "    for sample in token_list:\n",
    "        sample = list(set(sample) - set(tokens_to_be_removed))\n",
    "        token_list_.append(sample)\n",
    "\n",
    "    return token_list_\n",
    "\n",
    "\n",
    "def token_restructuring(token_list: list):\n",
    "    \"\"\"Reduce a nested list of tokens to a single list (1D).\n",
    "    \n",
    "    Args:\n",
    "        token_list (list): list to be work on.\n",
    "    \"\"\"\n",
    "    return reduce(lambda x, y: x + y, token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação da matriz de TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = tokenize_without_punctuation(X_train)\n",
    "top_tokens = top_tokens_stopwords(vocab)\n",
    "vocab = remove_specific_tokens(vocab,top_tokens)\n",
    "vocab = remove_specific_tokens(vocab,stopwords)\n",
    "text = [' '.join(tokens) for tokens in vocab]\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(text)\n",
    "X_train_tfidf = vectorizer.transform(text).toarray()\n",
    "X_train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_good_format = token_restructuring([list(row) for row in X_test])\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(X_test_good_format)\n",
    "X_test_tfidf = vectorizer.transform(X_test_good_format).toarray()\n",
    "X_test_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criaçãod do Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from numpy import genfromtxt\n",
    "import torch\n",
    "\n",
    "class ImdbDataset(Dataset):\n",
    "    def __init__(self, X, target):\n",
    "        super(ImdbDataset, self).__init__()\n",
    "\n",
    "        self.x = torch.tensor(X).type(torch.FloatTensor)\n",
    "        self.target = torch.tensor(target).type(torch.LongTensor)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "    \n",
    "        return len(self.x)\n",
    "  \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.target[index] \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando e Testando Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------->Testando Dataloader de treino<-------\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "tensor(1)\n",
      "------->Testando Dataloader de teste<-------\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "torch_ds_train = ImdbDataset(X_train_tfidf,y_train)\n",
    "torch_ds_test = ImdbDataset(X_test_tfidf,y_test)\n",
    "print(\"------->Testando Dataloader de treino<-------\")\n",
    "x1, y1 = torch_ds_train[0]\n",
    "print(x1)\n",
    "print(y1)\n",
    "print(\"------->Testando Dataloader de teste<-------\")\n",
    "x2, y2 = torch_ds_test[0]\n",
    "print(x2)\n",
    "print(y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salva modelo e outros artefatos\n",
    "\n",
    "Utiliza a função `save_model` do [SDK da PlatIAgro](https://platiagro.github.io/sdk/) para salvar modelos e outros artefatos.<br>\n",
    "Essa função torna estes artefatos disponíveis para o notebook de implantação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from platiagro import save_model\n",
    "\n",
    "save_model(torch_ds_train=torch_ds_train,\n",
    "           torch_ds_test=torch_ds_test)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "experiment_id": "ab814fda-ef4a-4cc6-9d83-a050094fb5aa",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "operator_id": "b8719065-0e88-4198-9635-33a3832f90e4"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
