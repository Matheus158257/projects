{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe Embeddings Bag - Experimento\n",
    "\n",
    "A component that convert a document collection on GloVe Bag of Embeddings features\n",
    "Este notebook apresenta:\n",
    "- como usar o [SDK](https://platiagro.github.io/sdk/) para carregar datasets, salvar modelos e outros artefatos.\n",
    "- como declarar parâmetros e usá-los para criar componentes reutilizáveis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare parâmetros e hiperparâmetros para o modelo\n",
    "Os componentes podem declarar (e usar) estes parâmetros como padrão:\n",
    "- dataset\n",
    "- target\n",
    "\n",
    "Use estes parâmetros para carregar/salvar conjutos de dados, modelos, métricas e figuras com a ajuda do [SDK da PlatIAgro](https://platiagro.github.io/sdk/). <br>\n",
    "É possível também declarar parâmetros personalizados para serem definidos ao executar um experimento. \n",
    "\n",
    "Selecione os hiperparâmetros e seus respectivos valores para serem usados ao treinar o modelo:\n",
    "- language\n",
    "\n",
    "Estes parâmetros são alguns dos oferecidos pela classe do modelo, você também pode utilizar outros existentes. <br>\n",
    "Dê uma olhada nos [parâmetros do modelo](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn-impute-simpleimputer) para mais informações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# parâmetros\n",
    "dataset = \"imdb.csv\" #@param {type:\"string\"}\n",
    "target = \"label\" #@param {type:\"feature\", label:\"Atributo alvo\", description:\"Seu modelo será treinado para prever os valores do alvo.\"}\n",
    "text = \"text\" #@param {type:\"string\", label:\"Texto alvo\", description:\"Nome da coluna do texto alvo pertencente ao dataset.\"}\n",
    "language = \"english\" #@param [\"portuguese\", \"english\"] {type:\"string\", label:\"Linguagem\", description:\"Linguagem da qual os stopwords pertencem. Deve ser a mesma utilizada no dataset.\"}\n",
    "batch_size = 100\n",
    "\n",
    "# selected features to perform the model\n",
    "filter_type = \"incluir\" #@param [\"incluir\",\"remover\"] {type:\"string\",multiple:false,label:\"Modo de seleção das features\",description:\"Se deseja informar quais features deseja incluir no modelo, selecione a opção 'incluir'. Caso deseje informar as features que não devem ser utilizadas, selecione 'remover'. \"}\n",
    "model_features = \"text\" #@param {type:\"feature\",multiple:true,label:\"Features para incluir/remover no modelo\",description:\"Seu modelo será feito considerando apenas as features selecionadas. Caso nada seja especificado, todas as features serão utilizadas\"}\n",
    "\n",
    "# features to apply One Hot Encoder\n",
    "one_hot_features = \"\" #@param {type:\"feature\",multiple:true,label:\"Features para fazer codificação one-hot\", description: \"Seu modelo utilizará a codificação one-hot para as features selecionadas. As demais features categóricas serão codificadas utilizando a codificação ordinal.\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acesso ao conjunto de dados\n",
    "\n",
    "O conjunto de dados utilizado nesta etapa será o mesmo carregado através da plataforma.<br>\n",
    "O tipo da variável retornada depende do arquivo de origem:\n",
    "- [pandas.DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) para CSV e compressed CSV: .csv .csv.zip .csv.gz .csv.bz2 .csv.xz\n",
    "- [Binary IO stream](https://docs.python.org/3/library/io.html#binary-i-o) para outros tipos de arquivo: .jpg .wav .zip .h5 .parquet etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>Un-bleeping-believable! Meg Ryan doesn't even ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>This is a extremely well-made film. The acting...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>Every once in a long while a movie will come a...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>Name just says it all. I watched this movie wi...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>This movie succeeds at being one of the most u...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>negative</td>\n",
       "      <td>There are many different versions of this one ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>positive</td>\n",
       "      <td>Once upon a time Hollywood produced live-actio...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>negative</td>\n",
       "      <td>Wenders was great with Million $ Hotel.I don't...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>negative</td>\n",
       "      <td>Although a film with Bruce Willis is always wo...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>positive</td>\n",
       "      <td>A compelling, honest, daring, and unforgettabl...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        label                                               text  is_valid\n",
       "0    negative  Un-bleeping-believable! Meg Ryan doesn't even ...     False\n",
       "1    positive  This is a extremely well-made film. The acting...     False\n",
       "2    negative  Every once in a long while a movie will come a...     False\n",
       "3    positive  Name just says it all. I watched this movie wi...     False\n",
       "4    negative  This movie succeeds at being one of the most u...     False\n",
       "..        ...                                                ...       ...\n",
       "995  negative  There are many different versions of this one ...      True\n",
       "996  positive  Once upon a time Hollywood produced live-actio...      True\n",
       "997  negative  Wenders was great with Million $ Hotel.I don't...      True\n",
       "998  negative  Although a film with Bruce Willis is always wo...      True\n",
       "999  positive  A compelling, honest, daring, and unforgettabl...      True\n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(f'/tmp/data/{dataset}')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acesso aos metadados do conjunto de dados\n",
    "\n",
    "Utiliza a função `stat_dataset` do [SDK da PlatIAgro](https://platiagro.github.io/sdk/) para carregar metadados. <br>\n",
    "Por exemplo, arquivos CSV possuem `metadata['featuretypes']` para cada coluna no conjunto de dados (ex: categorical, numerical, or datetime)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from platiagro import stat_dataset\n",
    "\n",
    "metadata = stat_dataset(name=dataset)\n",
    "featuretypes = metadata[\"featuretypes\"]\n",
    "\n",
    "columns = df.columns.to_numpy()\n",
    "featuretypes = np.array(featuretypes)\n",
    "target_index = np.argwhere(columns == target)\n",
    "columns = np.delete(columns, target_index)\n",
    "featuretypes = np.delete(featuretypes, target_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remoção de linhas com valores faltantes no atributo alvo\n",
    "\n",
    "Caso haja linhas em que o atributo alvo contenha valores faltantes, é feita a remoção dos casos faltantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset = [target],inplace=True)\n",
    "y = df[target].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtragem das features \n",
    "\n",
    "Seleciona apenas as features que foram declaradas no parâmetro model_features. Se nenhuma feature for especificada, todo o conjunto de dados será utilizado para a modelagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if filter_type == 'incluir':\n",
    "    if len(model_features) >= 1:\n",
    "        columns_index = (np.where(np.isin(columns,model_features)))[0]\n",
    "        columns_index.sort()\n",
    "        columns_to_filter = columns[columns_index]\n",
    "        featuretypes = featuretypes[columns_index]\n",
    "    else:\n",
    "        columns_to_filter = columns\n",
    "else:\n",
    "    if len(model_features) >= 1:\n",
    "        columns_index = (np.where(np.isin(columns,model_features)))[0]\n",
    "        columns_index.sort()\n",
    "        columns_to_filter = np.delete(columns,columns_index)\n",
    "        featuretypes = np.delete(featuretypes,columns_index)\n",
    "    else:\n",
    "        columns_to_filter = columns\n",
    "\n",
    "# keep the features selected\n",
    "df_model = df[columns_to_filter]\n",
    "X = df_model.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divisão do datset em subconjuntos de treino e teste\n",
    "\n",
    "Subconjunto de Treino: amostras de dados usado para treinar o modelo (``fit``). <br>\n",
    "Subconjunto de Teste: a amostra de dados usada para fornecer uma avaliação imparcial de um modelo adequado ao conjunto de dados de treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapeamento = {'positive': True, 'negative': False}\n",
    "y_train = [mapeamento.get(i, i) for i in y_train]\n",
    "y_test = [mapeamento.get(i, i) for i in y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Busca por stopwords\n",
    "\n",
    "Stopwords (ou palavras de parada) são palavras que geralmente se referem às mais comuns em um idioma ou em um corpus. <br>\n",
    "Elas podem ser ignoradas com segurança sem sacrificar o significado da frase, pois são palarvas que não agregram muito significado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk --quiet\n",
    "import nltk\n",
    "\n",
    "# Download stopwords from nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get a list of stopwords for the defined language\n",
    "stopwords = nltk.corpus.stopwords.words(language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processamento do texto\n",
    "\n",
    "Funções auxiliares para processamento dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import sub\n",
    "\n",
    "\n",
    "def tokenize_without_punctuation(text_list: list = None):\n",
    "    \"\"\"Tokenize without ponctuation.\n",
    "\n",
    "    Args:\n",
    "        text_list (list): a list of texts to be used.\n",
    "\n",
    "    Returns:\n",
    "        A list of tokenized text without punctuation.\n",
    "    \"\"\"\n",
    "    tokenize_list = list()\n",
    "    punctuation_pattern = \"[^a-zA-Z0-9áéíóúÁÉÍÓÚâêîôÂÊÎÔãõÃÕçÇ ]\"\n",
    "    html_tag_pattern = \"<.*?>\"\n",
    "\n",
    "    for text in text_list:\n",
    "        text = sub(html_tag_pattern, ' ', text[0])\n",
    "        tokenize_list.append(sub(punctuation_pattern, ' ', text).split(' '))\n",
    "\n",
    "    return tokenize_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dos Embeddings e Visualização do GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download dos Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘glove.6B.zip’ already there; not retrieving.\n",
      "\n",
      "Archive:  glove.6B.zip\n",
      "  inflating: glove_dir/glove.6B.50d.txt  \n",
      "  inflating: glove_dir/glove.6B.100d.txt  \n",
      "  inflating: glove_dir/glove.6B.200d.txt  \n",
      "  inflating: glove_dir/glove.6B.300d.txt  \n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.debug(\"test\")\n",
    "\n",
    "!wget -nc http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip -o glove.6B.zip -d glove_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criação das Estruturas Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torchtext.vocab:Loading vectors from ./glove_dir/glove.6B.300d.txt.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400000, 300])\n",
      "Primeiras 20 palavras e seus índices: [('the', 0), (',', 1), ('.', 2), ('of', 3), ('to', 4), ('and', 5), ('in', 6), ('a', 7), ('\"', 8), (\"'s\", 9), ('for', 10), ('-', 11), ('that', 12), ('on', 13), ('is', 14), ('was', 15), ('said', 16), ('with', 17), ('he', 18), ('as', 19)]\n",
      "Primeiras 20 palavras: ['the', ',', '.', 'of', 'to', 'and', 'in', 'a', '\"', \"'s\", 'for', '-', 'that', 'on', 'is', 'was', 'said', 'with', 'he', 'as']\n",
      "Primeiro vetor: tensor([ 4.6560e-02,  2.1318e-01, -7.4364e-03, -4.5854e-01, -3.5639e-02,\n",
      "         2.3643e-01, -2.8836e-01,  2.1521e-01, -1.3486e-01, -1.6413e+00,\n",
      "        -2.6091e-01,  3.2434e-02,  5.6621e-02, -4.3296e-02, -2.1672e-02,\n",
      "         2.2476e-01, -7.5129e-02, -6.7018e-02, -1.4247e-01,  3.8825e-02,\n",
      "        -1.8951e-01,  2.9977e-01,  3.9305e-01,  1.7887e-01, -1.7343e-01,\n",
      "        -2.1178e-01,  2.3617e-01, -6.3681e-02, -4.2318e-01, -1.1661e-01,\n",
      "         9.3754e-02,  1.7296e-01, -3.3073e-01,  4.9112e-01, -6.8995e-01,\n",
      "        -9.2462e-02,  2.4742e-01, -1.7991e-01,  9.7908e-02,  8.3118e-02,\n",
      "         1.5299e-01, -2.7276e-01, -3.8934e-02,  5.4453e-01,  5.3737e-01,\n",
      "         2.9105e-01, -7.3514e-03,  4.7880e-02, -4.0760e-01, -2.6759e-02,\n",
      "         1.7919e-01,  1.0977e-02, -1.0963e-01, -2.6395e-01,  7.3990e-02,\n",
      "         2.6236e-01, -1.5080e-01,  3.4623e-01,  2.5758e-01,  1.1971e-01,\n",
      "        -3.7135e-02, -7.1593e-02,  4.3898e-01, -4.0764e-02,  1.6425e-02,\n",
      "        -4.4640e-01,  1.7197e-01,  4.6246e-02,  5.8639e-02,  4.1499e-02,\n",
      "         5.3948e-01,  5.2495e-01,  1.1361e-01, -4.8315e-02, -3.6385e-01,\n",
      "         1.8704e-01,  9.2761e-02, -1.1129e-01, -4.2085e-01,  1.3992e-01,\n",
      "        -3.9338e-01, -6.7945e-02,  1.2188e-01,  1.6707e-01,  7.5169e-02,\n",
      "        -1.5529e-02, -1.9499e-01,  1.9638e-01,  5.3194e-02,  2.5170e-01,\n",
      "        -3.4845e-01, -1.0638e-01, -3.4692e-01, -1.9024e-01, -2.0040e-01,\n",
      "         1.2154e-01, -2.9208e-01,  2.3353e-02, -1.1618e-01, -3.5768e-01,\n",
      "         6.2304e-02,  3.5884e-01,  2.9060e-02,  7.3005e-03,  4.9482e-03,\n",
      "        -1.5048e-01, -1.2313e-01,  1.9337e-01,  1.2173e-01,  4.4503e-01,\n",
      "         2.5147e-01,  1.0781e-01, -1.7716e-01,  3.8691e-02,  8.1530e-02,\n",
      "         1.4667e-01,  6.3666e-02,  6.1332e-02, -7.5569e-02, -3.7724e-01,\n",
      "         1.5850e-02, -3.0342e-01,  2.8374e-01, -4.2013e-02, -4.0715e-02,\n",
      "        -1.5269e-01,  7.4980e-02,  1.5577e-01,  1.0433e-01,  3.1393e-01,\n",
      "         1.9309e-01,  1.9429e-01,  1.5185e-01, -1.0192e-01, -1.8785e-02,\n",
      "         2.0791e-01,  1.3366e-01,  1.9038e-01, -2.5558e-01,  3.0400e-01,\n",
      "        -1.8960e-02,  2.0147e-01, -4.2110e-01, -7.5156e-03, -2.7977e-01,\n",
      "        -1.9314e-01,  4.6204e-02,  1.9971e-01, -3.0207e-01,  2.5735e-01,\n",
      "         6.8107e-01, -1.9409e-01,  2.3984e-01,  2.2493e-01,  6.5224e-01,\n",
      "        -1.3561e-01, -1.7383e-01, -4.8209e-02, -1.1860e-01,  2.1588e-03,\n",
      "        -1.9525e-02,  1.1948e-01,  1.9346e-01, -4.0820e-01, -8.2966e-02,\n",
      "         1.6626e-01, -1.0601e-01,  3.5861e-01,  1.6922e-01,  7.2590e-02,\n",
      "        -2.4803e-01, -1.0024e-01, -5.2491e-01, -1.7745e-01, -3.6647e-01,\n",
      "         2.6180e-01, -1.2077e-02,  8.3190e-02, -2.1528e-01,  4.1045e-01,\n",
      "         2.9136e-01,  3.0869e-01,  7.8864e-02,  3.2207e-01, -4.1023e-02,\n",
      "        -1.0970e-01, -9.2041e-02, -1.2339e-01, -1.6416e-01,  3.5382e-01,\n",
      "        -8.2774e-02,  3.3171e-01, -2.4738e-01, -4.8928e-02,  1.5746e-01,\n",
      "         1.8988e-01, -2.6642e-02,  6.3315e-02, -1.0673e-02,  3.4089e-01,\n",
      "         1.4106e+00,  1.3417e-01,  2.8191e-01, -2.5940e-01,  5.5267e-02,\n",
      "        -5.2425e-02, -2.5789e-01,  1.9127e-02, -2.2084e-02,  3.2113e-01,\n",
      "         6.8818e-02,  5.1207e-01,  1.6478e-01, -2.0194e-01,  2.9232e-01,\n",
      "         9.8575e-02,  1.3145e-02, -1.0652e-01,  1.3510e-01, -4.5332e-02,\n",
      "         2.0697e-01, -4.8425e-01, -4.4706e-01,  3.3305e-03,  2.9264e-03,\n",
      "        -1.0975e-01, -2.3325e-01,  2.2442e-01, -1.0503e-01,  1.2339e-01,\n",
      "         1.0978e-01,  4.8994e-02, -2.5157e-01,  4.0319e-01,  3.5318e-01,\n",
      "         1.8651e-01, -2.3622e-02, -1.2734e-01,  1.1475e-01,  2.7359e-01,\n",
      "        -2.1866e-01,  1.5794e-02,  8.1754e-01, -2.3792e-02, -8.5469e-01,\n",
      "        -1.6203e-01,  1.8076e-01,  2.8014e-02, -1.4340e-01,  1.3139e-03,\n",
      "        -9.1735e-02, -8.9704e-02,  1.1105e-01, -1.6703e-01,  6.8377e-02,\n",
      "        -8.7388e-02, -3.9789e-02,  1.4184e-02,  2.1187e-01,  2.8579e-01,\n",
      "        -2.8797e-01, -5.8996e-02, -3.2436e-02, -4.7009e-03, -1.7052e-01,\n",
      "        -3.4741e-02, -1.1489e-01,  7.5093e-02,  9.9526e-02,  4.8183e-02,\n",
      "        -7.3775e-02, -4.1817e-01,  4.1268e-03,  4.4414e-01, -1.6062e-01,\n",
      "         1.4294e-01, -2.2628e+00, -2.7347e-02,  8.1311e-01,  7.7417e-01,\n",
      "        -2.5639e-01, -1.1576e-01, -1.1982e-01, -2.1363e-01,  2.8429e-02,\n",
      "         2.7261e-01,  3.1026e-02,  9.6782e-02,  6.7769e-03,  1.4082e-01,\n",
      "        -1.3064e-02, -2.9686e-01, -7.9913e-02,  1.9500e-01,  3.1549e-02,\n",
      "         2.8506e-01, -8.7461e-02,  9.0611e-03, -2.0989e-01,  5.3913e-02])\n"
     ]
    }
   ],
   "source": [
    "#Caso não possua instalado, instala antes de importar\n",
    "try:\n",
    "    from torchtext.vocab import GloVe\n",
    "except ModuleNotFoundError:\n",
    "    !pip install torchtext --quiet\n",
    "    from torchtext.vocab import GloVe\n",
    "    \n",
    "#Definição das variáveis da Bag Of Embeddings\n",
    "glove_dim = 300\n",
    "glove = GloVe(name='6B', dim=glove_dim, cache='./glove_dir')\n",
    "glove_vectors = glove.vectors\n",
    "glove_vocab = glove.stoi\n",
    "glove_words = glove.itos\n",
    "print(glove.vectors.shape)\n",
    "print('Primeiras 20 palavras e seus índices:', list(glove_vocab.items())[:20])\n",
    "print('Primeiras 20 palavras:',glove_words[:20])\n",
    "print('Primeiro vetor:', list(glove_vectors)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construção da matriz de Glove Embeddings Bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[357, 328, 9679, 11089, 1005, 1592, 116, 46349, 662, 39086, 117, 530, 69, 1740, 2219, 2772, 222, 169, 3573, 11068, 720, 389, 389, 6614, 2532, 1005, 63173, 20333, 63903, 8589, 143, 2301, 81189, 1056, 1005, 116, 24056, 2905, 6616, 357, 328, 756, 254, 6107, 4881, 48, 988, 2543, 732, 779, 79, 8829, 26926, 48, 4662, 94, 3162, 137, 1502, 823, 7299, 47845, 357, 328, 756, 1005, 54, 1716, 1247, 24056, 2905, 357, 328, 1420]\n",
      "['small', 'town', 'instantly', 'relate', 'movie', 'era', 'made', 'townsfolk', 'look', 'uncomfortably', 'like', 'lot', 'people', 'grew', 'plot', 'yes', 'going', 'get', 'nominated', 'anytime', 'soon', 'point', 'point', 'suspend', 'reality', 'movie', 'aplenty', 'greedy', 'uncaring', 'banker', 'well', 'meaning', 'dimwitted', 'deputy', 'movie', 'made', 'poke', 'fun', 'genre', 'small', 'town', 'living', 'best', 'smile', 'sight', 'one', 'growing', 'considering', 'technology', 'available', 'time', 'pleasant', 'romp', 'one', 'childhood', 'could', 'sit', 'back', 'afternoon', 'hand', 'laugh', 'foibles', 'small', 'town', 'living', 'movie', 'would', 'watch', 'reason', 'poke', 'fun', 'small', 'town', 'ways']\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "\n",
    "def build_glove_matrix(X,step,stopwords=stopwords):\n",
    "    X = tokenize_without_punctuation(X)\n",
    "    glove_matrix = []\n",
    "    word_filtered_matrix = []\n",
    "    for token_line in X:\n",
    "        if step == 'train':\n",
    "            token_phrase = [glove_vocab[word] for word in token_line if (word in glove_vocab and word not in stopwords)]\n",
    "            filtered_words =  [word for word in token_line if (word in glove_vocab and word not in stopwords)]\n",
    "        elif step == 'test':\n",
    "            token_phrase = [glove_vocab[word] for word in token_line if (word in glove_vocab)]\n",
    "            filtered_words = [word for word in token_line if (word in glove_vocab)]\n",
    "        word_filtered_matrix.append(filtered_words)\n",
    "        glove_matrix.append(token_phrase)\n",
    "  \n",
    "    return glove_matrix , word_filtered_matrix\n",
    "\n",
    "#listas de listas construidas\n",
    "X_train_glove_ids,X_train_glove_words  = build_glove_matrix(X_train,'train')\n",
    "X_test_glove_ids,X_test_glove_words = build_glove_matrix(X_test,'test')\n",
    "\n",
    "print(X_train_glove_ids[0])\n",
    "print(X_train_glove_words[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criaçãod do Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from numpy import genfromtxt\n",
    "import torch\n",
    "\n",
    "class ImdbDataset(Dataset):\n",
    "    def __init__(self, X, X_words,target):\n",
    "        super(ImdbDataset, self).__init__()\n",
    "\n",
    "        self.x = [torch.tensor(line).type(torch.LongTensor) for line in X ]\n",
    "        self.target = torch.tensor(target).type(torch.LongTensor) \n",
    "        self.words = X_words\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "    \n",
    "        return len(self.x)\n",
    "  \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.words[index], self.target[index]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando e Testando Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------->Testando Dataset de treino<-------\n",
      "tensor([  357,   328,  9679, 11089,  1005,  1592,   116, 46349,   662, 39086,\n",
      "          117,   530,    69,  1740,  2219,  2772,   222,   169,  3573, 11068,\n",
      "          720,   389,   389,  6614,  2532,  1005, 63173, 20333, 63903,  8589,\n",
      "          143,  2301, 81189,  1056,  1005,   116, 24056,  2905,  6616,   357,\n",
      "          328,   756,   254,  6107,  4881,    48,   988,  2543,   732,   779,\n",
      "           79,  8829, 26926,    48,  4662,    94,  3162,   137,  1502,   823,\n",
      "         7299, 47845,   357,   328,   756,  1005,    54,  1716,  1247, 24056,\n",
      "         2905,   357,   328,  1420])\n",
      "['small', 'town', 'instantly', 'relate', 'movie', 'era', 'made', 'townsfolk', 'look', 'uncomfortably', 'like', 'lot', 'people', 'grew', 'plot', 'yes', 'going', 'get', 'nominated', 'anytime', 'soon', 'point', 'point', 'suspend', 'reality', 'movie', 'aplenty', 'greedy', 'uncaring', 'banker', 'well', 'meaning', 'dimwitted', 'deputy', 'movie', 'made', 'poke', 'fun', 'genre', 'small', 'town', 'living', 'best', 'smile', 'sight', 'one', 'growing', 'considering', 'technology', 'available', 'time', 'pleasant', 'romp', 'one', 'childhood', 'could', 'sit', 'back', 'afternoon', 'hand', 'laugh', 'foibles', 'small', 'town', 'living', 'movie', 'would', 'watch', 'reason', 'poke', 'fun', 'small', 'town', 'ways']\n",
      "tensor(1)\n",
      "------->Testando Dataset de teste<-------\n",
      "tensor([    14,  10221,     19,    645,    557,      7,   1340,   2692,     12,\n",
      "          3322,      3,      7,    281,      3,   3855,    236,    208,     13,\n",
      "            48,   1671,    122,      3,      7,    142,  24853,      6,    853,\n",
      "            37,   4736,      3,      0,    523,   2919,     66,      4,     30,\n",
      "           871,    440,   1209,     73,    414,     22,     58,     30,    804,\n",
      "            19,      0,    853,     12,      0,    608,      6,    236,   1942,\n",
      "            14,   8485,      4,    795,    606,     14,   4592,  15606,      5,\n",
      "           191,    333,  25157,      0,    371,     25,  10372,     68,   1340,\n",
      "          1562,    853,     48,    591,     86,   2159,    275,   3687,  12216,\n",
      "             4,      0,     76,     19,     48,      3,      0,   5520,  10214,\n",
      "          6545,     13,      7,    592,  14734,   1632,  10417,   6865,   2494,\n",
      "             5,     20,   1534,     36,    120,      0,   3697,      3,     12,\n",
      "           907,      0,  12216,  44252,   1534,      7,    530,      3,  12120,\n",
      "            74, 135262,  31631,    403,      5,      7,    208,      3,   2153,\n",
      "          6475,  30761,    100,     12,    236,     48,     14,      6,     77,\n",
      "          1380,   9683,      6,    170,   6787,     20,   1534,    555,      4,\n",
      "          5020,      0,   2173,    165,    143,    106,      0,    156,     36,\n",
      "           113,      3,      0,   2169,   6877,      3,      0,    523,     34,\n",
      "           871,    113,      3,      0,    916,      3,      0,   2692,     19,\n",
      "             7,   1115,   1192,    654,    390,     30,   1446,      4,   7667,\n",
      "             0,    179,     12,      0,    281,   6545,     13,   1192,   2153,\n",
      "            22,   1192,    246,   8760,      3,    158,  24995,      0,    281,\n",
      "          1740,     13,    285,     21,      0,    156,     15,   1689,  16770,\n",
      "            20,   1534,      7,   5279,   1100,     12,      0,     42,    442,\n",
      "           116,      0,   4913,      3,  14558,      3,     37,      3,   2590,\n",
      "             0,    254,   2692,      3,      0,     62,     10,     63,   1534,\n",
      "           333,   1446,  27620,     13,    901,    187,      5,      0,   1547,\n",
      "          7488,      7,   1492,      3,  10249,     12,     14,  88053,   5407,\n",
      "            14,      6,    853,     36,  21168,     34,     20,     14,  44004,\n",
      "             5,     48,     54,    824,     12,      0,    191,    254,      0,\n",
      "            40,      4,    901,     54,     30,    645,      7,    333,     56,\n",
      "          7542,      5,   1903])\n",
      "['is', 'billed', 'as', 'something', 'special', 'a', 'crime', 'drama', 'that', 'consists', 'of', 'a', 'series', 'of', 'episodes', 'each', 'set', 'on', 'one', 'particular', 'day', 'of', 'a', 'police', 'enquiry', 'in', 'fact', 'this', 'element', 'of', 'the', 'story', 'turns', 'out', 'to', 'be', 'rather', 'less', 'significant', 'than', 'might', 'at', 'first', 'be', 'thought', 'as', 'the', 'fact', 'that', 'the', 'action', 'in', 'each', 'episode', 'is', 'confined', 'to', '24', 'hours', 'is', 'hardly', 'noticeable', 'and', 'very', 'little', 'distinguishes', 'the', 'program', 'from', 'countless', 'other', 'crime', 'stories', 'fact', 'one', 'almost', 'can', 't', 'help', 'drawing', 'comparisons', 'to', 'the', 'last', 'as', 'one', 'of', 'the', 'sub', 'plots', 'focuses', 'on', 'a', 'single', 'cynical', 'female', 'cop', 'approaching', 'retirement', 'and', 'it', 's', 'not', 'just', 'the', 'absence', 'of', 'that', 'makes', 'the', 'comparisons', 'unfavourable', 's', 'a', 'lot', 'of', 'earnest', 'over', 'emoting', 'manipulative', 'music', 'and', 'a', 'set', 'of', 'characters', 'seemingly', 'contrived', 'so', 'that', 'each', 'one', 'is', 'in', 'some', 'sense', 'sympathetic', 'in', 'another', 'suspicious', 'it', 's', 'possible', 'to', 'guess', 'the', 'guilty', 'party', 'well', 'before', 'the', 'end', 'not', 'because', 'of', 'the', 'internal', 'dynamic', 'of', 'the', 'story', 'but', 'rather', 'because', 'of', 'the', 'construction', 'of', 'the', 'drama', 'as', 'a', 'whole', 'certain', 'things', 'must', 'be', 'true', 'to', 'justify', 'the', 'way', 'that', 'the', 'series', 'focuses', 'on', 'certain', 'characters', 'at', 'certain', 'times', 'spite', 'of', 'these', 'failings', 'the', 'series', 'grew', 'on', 'me', 'by', 'the', 'end', 'was', 'quite', 'gripped', 'it', 's', 'a', 'sad', 'sign', 'that', 'the', 'which', 'once', 'made', 'the', 'likes', 'of', 'boasted', 'of', 'this', 'of', 'possibly', 'the', 'best', 'drama', 'of', 'the', 'year', 'for', 'there', 's', 'little', 'true', 'originality', 'on', 'offer', 'here', 'and', 'the', 'claim', 'reveals', 'a', 'lack', 'of', 'ambition', 'that', 'is', 'dreadfully', 'disappointing', 'is', 'in', 'fact', 'not', 'rubbish', 'but', 'it', 'is', 'formulaic', 'and', 'one', 'would', 'hope', 'that', 'the', 'very', 'best', 'the', 'had', 'to', 'offer', 'would', 'be', 'something', 'a', 'little', 'more', 'innovative', 'and', 'fresh']\n",
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "torch_ds_train = ImdbDataset(X_train_glove_ids,X_train_glove_words,y_train)\n",
    "torch_ds_test = ImdbDataset(X_test_glove_ids,X_test_glove_words,y_test)\n",
    "print(\"------->Testando Dataset de treino<-------\")\n",
    "x1,xw1, y1 = torch_ds_train[0]\n",
    "print(x1)\n",
    "print(xw1)\n",
    "print(y1)\n",
    "print(\"------->Testando Dataset de teste<-------\")\n",
    "x2,xw2, y2 = torch_ds_test[0]\n",
    "print(x2)\n",
    "print(xw2)\n",
    "print(y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Collatae serve par criar um offset em caso de entradas variáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    #import pdb;pdb.set_trace()\n",
    "    # len soma de todas as palavras\n",
    "    lista_words = []\n",
    "    [lista_words.extend(item[1]) for item in batch]\n",
    "\n",
    "    # len soma de todas as palavras\n",
    "    lista_words_ids = [item[0] for item in batch]\n",
    "    lista_words_ids_vector = torch.cat(lista_words_ids)\n",
    "\n",
    "    #len batch_size\n",
    "    target = [item[2] for item in batch]\n",
    "    target = torch.stack(target)\n",
    "\n",
    "    # len batch_size\n",
    "    offsets = [0] + [len(entry) for entry in lista_words_ids]\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    \n",
    "    return lista_words_ids_vector, target , offsets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(torch_ds_train, batch_size=batch_size, shuffle=True, collate_fn=my_collate)\n",
    "test_loader = DataLoader(torch_ds_test, batch_size=batch_size, shuffle=False, collate_fn=my_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testando Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------->Testando Dataloader de treino<-------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([16058,   219,   333,  ...,  1588,   117,   456]),\n",
       " tensor([1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,\n",
       "         0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "         1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "         1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "         0, 0, 1, 1]),\n",
       " tensor([   0,  166,  278,  463,  687,  740,  789,  940, 1019, 1099, 1182, 1318,\n",
       "         1375, 1588, 1658, 1685, 1790, 2156, 2246, 2297, 2680, 2794, 2844, 2923,\n",
       "         3001, 3050, 3108, 3177, 3235, 3274, 3440, 3500, 3608, 3939, 4028, 4148,\n",
       "         4205, 4247, 4315, 4378, 4570, 4597, 4612, 4687, 4845, 4879, 4930, 5304,\n",
       "         5446, 5490, 5559, 5634, 5700, 5807, 5854, 5875, 5985, 6037, 6122, 6242,\n",
       "         6335, 6444, 6525, 6580, 6635, 6691, 6828, 6932, 7026, 7086, 7116, 7296,\n",
       "         7349, 7459, 7525, 7580, 7656, 7899, 7934, 7978, 8043, 8129, 8194, 8392,\n",
       "         8584, 8644, 8757, 8818, 8902, 8958, 9059, 9263, 9347, 9379, 9408, 9502,\n",
       "         9520, 9663, 9733, 9884]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"------->Testando Dataloader de treino<-------\")\n",
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------->Testando Dataloader de teste<-------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([   14, 10221,    19,  ...,   308,    13,    20]),\n",
       " tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,\n",
       "         0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "         1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
       "         1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "         1, 0, 0, 0]),\n",
       " tensor([    0,   282,   458,   671,   870,  1128,  1238,  1400,  1675,  1805,\n",
       "          1877,  2068,  2401,  2520,  2982,  3071,  3376,  3432,  3539,  3728,\n",
       "          3877,  4086,  4450,  4725,  5174,  5220,  5265,  5887,  6376,  6780,\n",
       "          7127,  7242,  7858,  7976,  8334,  8517,  8626,  9133,  9524,  9627,\n",
       "          9956, 10107, 10460, 10601, 10745, 10845, 10986, 11192, 11335, 11436,\n",
       "         11551, 11884, 12048, 12350, 12429, 12557, 12617, 12653, 12836, 12987,\n",
       "         13117, 13403, 14176, 14229, 14334, 14461, 14510, 14673, 14792, 14826,\n",
       "         15094, 15563, 15626, 15708, 15858, 16070, 16365, 16521, 16637, 16906,\n",
       "         17110, 17277, 17364, 18039, 18188, 18227, 18615, 18892, 19118, 19251,\n",
       "         19405, 19755, 19986, 20340, 20389, 20596, 20787, 20896, 21325, 21441]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"------->Testando Dataloader de teste<-------\")\n",
    "next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salva modelo e outros artefatos\n",
    "\n",
    "Utiliza a função `save_model` do [SDK da PlatIAgro](https://platiagro.github.io/sdk/) para salvar modelos e outros artefatos.<br>\n",
    "Essa função torna estes artefatos disponíveis para o notebook de implantação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): server.anonymous:80\n",
      "DEBUG:urllib3.connectionpool:http://server.anonymous:80 \"GET /notebook/anonymous/server/api/sessions HTTP/1.1\" 200 1380\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): server.anonymous:80\n",
      "DEBUG:urllib3.connectionpool:http://server.anonymous:80 \"GET /notebook/anonymous/server/api/sessions HTTP/1.1\" 200 1380\n",
      "/opt/conda/lib/python3.7/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n",
      "DEBUG:urllib3.connectionpool:http://minio-service.kubeflow:9000 \"PUT /anonymous/ HTTP/1.1\" 409 None\n",
      "DEBUG:urllib3.connectionpool:http://minio-service.kubeflow:9000 \"PUT /anonymous/experiments/d930015c-45a2-42eb-912a-541ce198cee6/operators/fb743ef0-6ddd-4bc1-8d14-ec82ce0ef401/model.joblib HTTP/1.1\" 200 0\n"
     ]
    }
   ],
   "source": [
    "from platiagro import save_model\n",
    "\n",
    "save_model(train_loader=train_loader,\n",
    "           test_loader=test_loader)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "experiment_id": "d930015c-45a2-42eb-912a-541ce198cee6",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "operator_id": "fb743ef0-6ddd-4bc1-8d14-ec82ce0ef401"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
